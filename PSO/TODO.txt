next:
add/recover and finish Dimension 
recover everything in AbstractSpace
recover everything from Particle
recover Neighbourhood
think of how to perform upgradeable log writing (observer?)
think of how to proceed communicating the data in the PSO
recover serialization in Particle
serialize the neighbourhood(s)
recover everything in PSO

ADD TO DOCUMENTATION:
-> POW must create PsoParameters, then call params.<method>(psoParams) before
passing it to PSO
-> space and fitness are not bcasted. space - not needed. fitness - modify
printer (will be a new class)


PSO (c++):
-> continue implementing according to comments in the methods
-> recover all the commented stuff

Particle:
-> recover commented stuff

Neighbourhood and derived classes
-> remove Swarm as property of N'hood, put Swarm to be parameter for methods
-> serialize N'hoods
-> get rid of all TODOs within these classes after Swarm is implemented

Space / Dimension
-> AbstractSpace - implement checkBoundaries (first needs to have idea of what
should be the input parameter - Particle or vector, but for that, I need to
know how PSO and Particle will work)
-> add method that converts high/low/... to Dimension classes (think if this
will be in c++ or python)
-> add a boolean that checks if this method is called by a module programmer
-> in pow, check if this boolean is still false, and if yes, throw Error

SWIG module.i:
-> add mpi support
-> if needed, add interface from c++ to python string

MPI: after testing regular code, add MPI support
-> add MPI
-> Master/slave architecture
-> implement skeleton/content

TEST:
-> PsoParameters with Parameters integration (do they fill correctly)
-> test Dimension (more precisely, PeriodicDimension and ReflexiveDimension)

IF THERE IS TIME:
-> kick and reseed
-> repellers
-> save/load to enable restart from file

##############################################################################
# DONE:                                                                      #
##############################################################################

General:
(22/23.5.)+> FIRST COMPILE AND RUN! although PSO is basically empty, it
finally integrates together

PSO:
(3.5.)+> skeleton created
(3.5.)+> make params.inertia part of PSO (or swarm) (instead of params)

Particle
(29.4.)+> implemented basic version

Swarm
(29.4.)+> implemented basic version
(10.5.)+> serialization added

Neighbourhood and derived:
(29.4.)+> implemented basic versions

Parser/params:
(12.4.)+> think of methods to fill in the c++ map from Python when
initializing
(12.4.)+> think of how to update when something is set in both directions
(20.5.)+> throughout Parser class, added calls that integrate with c++
BaseParameters, including all conversion between python and c++ types
(22.5.)+> renamed BaseParameters to PsoParameters; this class is no more a
superclass of Parser - it was not possible to broadcast it because it could
not be pickled; PsoParameters will serve as a standalone c++ class with its
own maps, and all the parameters from params will be copied there

Space / Dimension
(15.4.)+> Dimension.checkBoundaries() updates the particle. check if it needs
to return a new particle, or some vectors
(19.4.)+>AbstractSpace / Space will then have a list of references to
dimensions, instead the list of high/low/cell_size/...

AbstractFitness / BaseFitness:
(old)+> connect to Python: AbstractFitness <- BaseFitness <- Fitness
(17.5.)+> BaseFitness implements virtual call_evaluate from c++ to connect it
with evaluate (need Particle)
(17.5.)-> Fitness is then implemented by the user. Added Simple.py to test
(17.5.)-> in BaseFitness check the consistency of the called methods for
array/list conversion and particle methods after they are implemented

MPI
(6.5.)+> find out a way to send serialized objects (considering that MPI was
initialized in Python):
"""
Boost.MPI is benefitial because it provides a way of passing serialized
objects. In "native" MPI, one needs to define special typemaps in order to
pass user-defined datatypes, which seems very painful and nonupgradeable.

The problem with Boost.MPI seemed to be that it is difficult to connect to
Python via SWIG, because Boost has its own library for mapping C++ to Python
(Boost.Python). Nevertheless, it seems that there is a way of connecting the
two.

Boost.MPI is initailized with two classes: boost::mpi::environment and
boost::mpi::communicator (when it's the only one used in a separate c++ main).
The good thing is that boost::mpi::communicator can be initialized with
existing MPI_Comm, and in this case boost::mpi::environment is not needed. In
this case, I hope that MPI_Comm that is initialized in Python and is sent
through SWIG can be used to initialize boost::mpi::communicator.
"""
(9.5.)+> done test for passing MPI.comm from python, through SWIG, to c++, and
finally initialized with boost::mpi
"""
This is very important because of two reasons:
1) There is a constructor for boost::mpi::communicator that accepts MPI_Comm.
However, it was very unclear if it will be possible to initialize
boost::mpi::communicator with mpi4py's MPI_comm sent from Python.
2) After this test was programmed, I had extreme doubts if it will be possible
to compile everything through SWIG. Luckily, there is a way to do so.
"""

SWIG:
(15.4.)+> do basic one
(17.5.)+> list-to-vector adaptor implemented
(17.5.)+> update polymorphism for every class
(17.5.)+> added pyString to cppString functionality
(20.5.)+> add templated vector methods from test/vector_in_python

REPORT:
(14.5.)+> first class diagram in violet
